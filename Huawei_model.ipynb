{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from mindspore import context\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Recycle Grad-CAM model')\n",
    "parser.add_argument('--device_target', type=str, default=\"Ascend\", choices=['Ascend', 'GPU', 'CPU'])\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=args.device_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "from mindspore.dataset.vision import Inter\n",
    "from mindspore import dtype as mstype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.common.initializer import TruncatedNormal\n",
    "from mindspore.ops import operations as P\n",
    "\n",
    "\n",
    "def weight_variable():\n",
    "    \"\"\"Weight variable.\"\"\"\n",
    "    return TruncatedNormal(0.02)\n",
    "\n",
    "\n",
    "class Conv2dBlock(nn.Cell):\n",
    "    \"\"\"\n",
    "     Basic convolutional block\n",
    "     Args:\n",
    "         in_channles (int): Input channel.\n",
    "         out_channels (int): Output channel.\n",
    "         kernel_size (int): Input kernel size. Default: 1\n",
    "         stride (int): Stride size for the first convolutional layer. Default: 1.\n",
    "         padding (int): Implicit paddings on both sides of the input. Default: 0.\n",
    "         pad_mode (str): Padding mode. Optional values are \"same\", \"valid\", \"pad\". Default: \"same\".\n",
    "      Returns:\n",
    "          Tensor, output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, pad_mode=\"same\"):\n",
    "        super(Conv2dBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, pad_mode=pad_mode, weight_init=weight_variable())\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Inception(nn.Cell):\n",
    "    \"\"\"\n",
    "    Inception Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n",
    "        super(Inception, self).__init__()\n",
    "        self.b1 = Conv2dBlock(in_channels, n1x1, kernel_size=1)\n",
    "        self.b2 = nn.SequentialCell([Conv2dBlock(in_channels, n3x3red, kernel_size=1),\n",
    "                                     Conv2dBlock(n3x3red, n3x3, kernel_size=3, padding=0)])\n",
    "        self.b3 = nn.SequentialCell([Conv2dBlock(in_channels, n5x5red, kernel_size=1),\n",
    "                                     Conv2dBlock(n5x5red, n5x5, kernel_size=3, padding=0)])\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n",
    "        self.b4 = Conv2dBlock(in_channels, pool_planes, kernel_size=1)\n",
    "        self.concat = P.Concat(axis=1)\n",
    "\n",
    "    def construct(self, x):\n",
    "        branch1 = self.b1(x)\n",
    "        branch2 = self.b2(x)\n",
    "        branch3 = self.b3(x)\n",
    "        cell = self.maxpool(x)\n",
    "        branch4 = self.b4(cell)\n",
    "        return self.concat((branch1, branch2, branch3, branch4))\n",
    "\n",
    "\n",
    "class GoogleNet(nn.Cell):\n",
    "    \"\"\"\n",
    "    Googlenet architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, include_top=True):\n",
    "        super(GoogleNet, self).__init__()\n",
    "        self.conv1 = Conv2dBlock(3, 64, kernel_size=7, stride=2, padding=0)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n",
    "\n",
    "        self.conv2 = Conv2dBlock(64, 64, kernel_size=1)\n",
    "        self.conv3 = Conv2dBlock(64, 192, kernel_size=3, padding=0)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n",
    "\n",
    "        self.block3a = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.block3b = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n",
    "\n",
    "        self.block4a = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.block4b = Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.block4c = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.block4d = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.block4e = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2, pad_mode=\"same\")\n",
    "\n",
    "        self.block5a = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.block5b = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        self.dropout = nn.Dropout(keep_prob=0.8)\n",
    "        self.include_top = include_top\n",
    "        if self.include_top:\n",
    "            self.mean = P.ReduceMean(keep_dims=True)\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.classifier = nn.Dense(1024, num_classes, weight_init=weight_variable(),\n",
    "                                       bias_init=weight_variable())\n",
    "\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"construct\"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.block3a(x)\n",
    "        x = self.block3b(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = self.block4a(x)\n",
    "        x = self.block4b(x)\n",
    "        x = self.block4c(x)\n",
    "        x = self.block4d(x)\n",
    "        x = self.block4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "\n",
    "        x = self.block5a(x)\n",
    "        x = self.block5b(x)\n",
    "        if not self.include_top:\n",
    "            return x\n",
    "\n",
    "        x = self.mean(x, (2, 3))\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GoogleNet(num_classes = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_opt = nn.Momentum(net.trainable_params(), learning_rate=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig\n",
    "# Set model saving parameters.\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=14, keep_checkpoint_max=20)\n",
    "# Use model saving parameters.\n",
    "ckpoint = ModelCheckpoint(prefix=\"recycle_huawei\", config=config_ck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "class RecycleData:\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        y_label = int(self.annotations.iloc[index, 1])\n",
    "\n",
    "        return (image, y_label)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "def create_dataset(data_path, batch_size=32, repeat_size=1,\n",
    "                   num_parallel_workers=1):\n",
    "    # Define the dataset.\n",
    "    train_ds = ds.GeneratorDataset(source = RecycleData(\"train.csv\", \"Train_data\"), column_names = [\"image\",\"label\"])\n",
    "\n",
    "    resize_height, resize_width = 224, 224\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "    rescale_nml = 1 / 0.3081\n",
    "    shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "    # Define the mapping to be operated.\n",
    "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR)\n",
    "    rescale_nml_op = CV.Rescale(rescale_nml, shift_nml)\n",
    "    rescale_op = CV.Rescale(rescale, shift)\n",
    "    hwc2chw_op = CV.HWC2CHW()\n",
    "    type_cast_op = C.TypeCast(mstype.int32)\n",
    "\n",
    "    # Use the map function to apply data operations to the dataset.\n",
    "    train_ds = train_ds.map(operations=type_cast_op, input_columns=\"label\", num_parallel_workers=num_parallel_workers)\n",
    "    train_ds = train_ds.map(operations=resize_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    train_ds = train_ds.map(operations=rescale_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    train_ds = train_ds.map(operations=rescale_nml_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    train_ds = train_ds.map(operations=hwc2chw_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "    # Perform shuffle and batch operations.\n",
    "    buffer_size = 1000\n",
    "    train_ds = train_ds.shuffle(buffer_size=buffer_size)\n",
    "    train_ds = train_ds.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = ds.GeneratorDataset(source = RecycleData(\"train.csv\", \"Train_data\"), column_names = [\"data\",\"label\"])\n",
    "\n",
    "# for data in dataset.create_dict_iterator():\n",
    "#         print(\"Image shape {}, label {}\".format(data['data'], data['label']))\n",
    "# iterator = iter(dataset)\n",
    "# x, y = next(iterator)\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library required for model training.\n",
    "from mindspore.nn import Accuracy\n",
    "from mindspore.train.callback import LossMonitor\n",
    "from mindspore import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def train_net(args, model, epoch_size, data_path, repeat_size, ckpoint_cb, sink_mode):\n",
    "    \"\"\"Define a training method.\"\"\"\n",
    "    # Load the training dataset.\n",
    "    ds_train = create_dataset(os.path.join(data_path, \"Train_data\"), 32, repeat_size)\n",
    "    model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor(14)], dataset_sink_mode=sink_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net(network, model, data_path):\n",
    "    \"\"\"Define a validation method.\"\"\"\n",
    "    ds_eval = create_dataset(os.path.join(data_path, \"Train_data\"))\n",
    "    acc = model.eval(ds_eval, dataset_sink_mode=False)\n",
    "    print(\"{}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = 15\n",
    "mnist_path = \"\"\n",
    "dataset_size = 1\n",
    "# model = Model(net, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()})\n",
    "# train_net(args, model, train_epoch, mnist_path, dataset_size, ckpoint, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "\n",
    "googlenet = GoogleNet(4)\n",
    "# Store model parameters in the parameter dictionary.\n",
    "param_dict = load_checkpoint(\"recycle_huawei-11_14.ckpt\")\n",
    "# Load parameters to the network.\n",
    "load_param_into_net(googlenet, param_dict)\n",
    "model = Model(googlenet, net_loss, metrics={\"accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9375}\n"
     ]
    }
   ],
   "source": [
    "test_net(googlenet, model, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_eval = create_dataset(os.path.join(\"\", \"Train_data\"))\n",
    "# image = io.imread(\"Train_data/Can/can001.jpg\")\n",
    "# import cv2\n",
    "# image = cv2.resize(image,(224,224))\n",
    "# val = model.predict(image.tolist())\n",
    "# print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mindspore import Tensor\n",
    "\n",
    "# Define a test dataset. If batch_size is set to 1, an image is obtained.\n",
    "ds_test = create_dataset(os.path.join(mnist_path, \"Train_data\"), batch_size=1).create_dict_iterator()\n",
    "data = next(ds_test)\n",
    "\n",
    "# `images` indicates the test image, and `labels` indicates the actual classification of the test image.\n",
    "images = data[\"image\"].asnumpy()\n",
    "labels = data[\"label\"].asnumpy()\n",
    "\n",
    "# Use the model.predict function to predict the classification of the image.\n",
    "output = model.predict(Tensor(data['image']))\n",
    "print(output)\n",
    "predicted = np.argmax(output.asnumpy(), axis=1)\n",
    "\n",
    "# Output the predicted classification and the actual classification.\n",
    "print(f'Predicted: \"{predicted[0]}\", Actual: \"{labels[0]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore_xai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import context, load_checkpoint, load_param_into_net\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "\n",
    "\n",
    "context.set_context(mode=context.PYNATIVE_MODE)\n",
    "\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "# load the trained classifier\n",
    "net = GoogleNet(num_classes)\n",
    "param_dict = load_checkpoint(\"recycle_huawei-11_14.ckpt\")\n",
    "load_param_into_net(net, param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "ds_sample = create_dataset(os.path.join(\"\", \"Test_data\"), batch_size=1).create_dict_iterator()\n",
    "data_sample = next(ds_sample)\n",
    "x = data_sample[\"image\"]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "from mindspore import Tensor\n",
    "from mindspore_xai.explanation import GradCAM\n",
    "\n",
    "# usually specify the last convolutional layer\n",
    "# print(net)\n",
    "grad_cam = GradCAM(net, layer=\"block5b\")\n",
    "\n",
    "# 5 is the class id of 'boat'\n",
    "saliency = grad_cam(x, targets=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(saliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image = cv2.imread('paper015.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
